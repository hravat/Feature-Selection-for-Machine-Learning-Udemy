{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterMethodConstantFeatures:\n",
    "    def __init__(self,data_path):\n",
    "        self.df=pd.read_csv(data_path)\n",
    "        \n",
    "    def dataframe_info(self):\n",
    "        self.df.info()\n",
    "        \n",
    "    def dataframe_stats(self):\n",
    "        print(self.df.describe().T)\n",
    "        \n",
    "        \n",
    "    def __train_test_split_fmb(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.df.drop(['target'],axis='columns'),\\\n",
    "                                                            self.df['target'],\\\n",
    "                                                            test_size=0.3, \\\n",
    "                                                            random_state = 0\n",
    "                                                           )\n",
    "        return  X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def __fit_summary(self,num_not_constant_features,num_constant_features,initial_shape_xtrain,initial_shape_xtest,X_train,X_test):        \n",
    "            print('----Fit Summary------')\n",
    "            print('---------------------')\n",
    "            print('A total of '+str(num_not_constant_features)+' features are not constant')\n",
    "            print('A total of '+str(num_constant_features)+' features are constant')\n",
    "            print('The train shape before fit is '+str(initial_shape_xtrain))\n",
    "            print('The train shape after fit is '+str(np.shape(X_test)))\n",
    "            print('The test shape before fit is '+str(initial_shape_xtest))\n",
    "            print('The test shape after fit is '+str(np.shape(X_train)))\n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "    def varinance_threshold(self):\n",
    "        X_train, X_test, y_train, y_test = self.__train_test_split_fmb()\n",
    "        \n",
    "        sel = VarianceThreshold(threshold=0)\n",
    "        sel.fit(X_train)  # fit finds the features with zero variance\n",
    "        \n",
    "        initial_shape_xtrain = np.shape(X_train)\n",
    "        initial_shape_xtest = np.shape(X_test)\n",
    "        \n",
    "        X_train = sel.transform(X_train)\n",
    "        X_test = sel.transform(X_test)\n",
    "        \n",
    "        # get_support is a boolean vector that indicates which features are retained\n",
    "        # if we sum over get_support, we get the number of features that are not constant \n",
    "        # True os constant False is not constant\n",
    "        \n",
    "        self.__fit_summary(np.sum(sel.get_support()),\n",
    "                           np.sum(~sel.get_support()),\n",
    "                           initial_shape_xtrain,\n",
    "                           initial_shape_xtest,\n",
    "                           X_train,\n",
    "                           X_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def pandas_std(self):\n",
    "        X_train, X_test, y_train, y_test = self.__train_test_split_fmb()   \n",
    "        consant_features =[features for features in X_train.columns if X_train[features].std() == 0 ]\n",
    "        \n",
    "        initial_shape_xtrain = np.shape(X_train)\n",
    "        initial_shape_xtest = np.shape(X_test)\n",
    "        initial_num_features = len(X_train.columns)\n",
    "        \n",
    "        \n",
    "        X_train.drop(consant_features,inplace=True,axis=1)\n",
    "        X_test.drop(consant_features,inplace=True,axis=1)\n",
    "        after_num_features = len(X_train.columns) \n",
    "        \n",
    "        self.__fit_summary(after_num_features,\n",
    "                           initial_num_features-after_num_features,\n",
    "                           initial_shape_xtrain,\n",
    "                           initial_shape_xtest,\n",
    "                           X_train,\n",
    "                           X_test)\n",
    "        \n",
    "    def pandas_nunique(self):\n",
    "        X_train, X_test, y_train, y_test = self.__train_test_split_fmb()   \n",
    "        consant_features =[features for features in X_train.columns if X_train[features].nunique() == 1]\n",
    "        \n",
    "        initial_shape_xtrain = np.shape(X_train)\n",
    "        initial_shape_xtest = np.shape(X_test)\n",
    "        initial_num_features = len(X_train.columns)\n",
    "        \n",
    "        \n",
    "        X_train.drop(consant_features,inplace=True,axis=1)\n",
    "        X_test.drop(consant_features,inplace=True,axis=1)\n",
    "        after_num_features = len(X_train.columns) \n",
    "        \n",
    "        self.__fit_summary(after_num_features,\n",
    "                           initial_num_features-after_num_features,\n",
    "                           initial_shape_xtrain,\n",
    "                           initial_shape_xtest,\n",
    "                           X_train,\n",
    "                           X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1 = FilterMethodConstantFeatures('../data/dataset_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Columns: 301 entries, var_1 to target\n",
      "dtypes: float64(127), int64(174)\n",
      "memory usage: 114.8 MB\n"
     ]
    }
   ],
   "source": [
    "obj1.dataframe_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count         mean           std  min  25%   50%  75%           max\n",
      "var_1    50000.0     0.002220      0.108145  0.0  0.0  0.00  0.0  9.000000e+00\n",
      "var_2    50000.0     0.000060      0.007746  0.0  0.0  0.00  0.0  1.000000e+00\n",
      "var_3    50000.0    15.593002   1280.571855  0.0  0.0  0.00  0.0  2.079013e+05\n",
      "var_4    50000.0     3.149633      2.740114  0.0  0.0  2.85  3.0  3.528000e+01\n",
      "var_5    50000.0   608.681764  10951.361737  0.0  0.0  0.00  0.0  4.455000e+05\n",
      "...          ...          ...           ...  ...  ...   ...  ...           ...\n",
      "var_297  50000.0     0.000000      0.000000  0.0  0.0  0.00  0.0  0.000000e+00\n",
      "var_298  50000.0     0.003060      0.078808  0.0  0.0  0.00  0.0  3.000000e+00\n",
      "var_299  50000.0    12.462960    832.417622  0.0  0.0  0.00  0.0  1.346667e+05\n",
      "var_300  50000.0  5683.960293  47364.820421  0.0  0.0  0.00  0.0  2.857673e+06\n",
      "target   50000.0     0.039820      0.195538  0.0  0.0  0.00  0.0  1.000000e+00\n",
      "\n",
      "[301 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "obj1.dataframe_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant features\n",
    "\n",
    "Constant features are those that show the same value, just one value, for all the observations of the dataset. In other words, the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target.\n",
    "\n",
    "Identifying and removing constant features is an easy first step towards feature selection and more easily interpretable machine learning models.\n",
    "\n",
    "\n",
    "To identify constant features, we can use the VarianceThreshold from Scikit-learn, or we can code it ourselves. If using the VarianceThreshold, all our variables need to be numerical. If we do it manually however, we can apply the code to both numerical and categorical variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VarianceThreshold from Scikit-learn\n",
    "\n",
    "The VarianceThreshold from sklearn provides a simple baseline approach to feature selection. It removes all features which variance doesnâ€™t meet a certain threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Fit Summary------\n",
      "---------------------\n",
      "A total of 266 features are not constant\n",
      "A total of 34 features are constant\n",
      "The train shape before fit is (35000, 300)\n",
      "The train shape after fit is (15000, 266)\n",
      "The test shape before fit is (15000, 300)\n",
      "The test shape after fit is (35000, 266)\n"
     ]
    }
   ],
   "source": [
    "obj1.varinance_threshold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual code 1: only works with numerical\n",
    "\n",
    "In the following cells, I will show an alternative to the VarianceThreshold transformer of sklearn, were we write the code to find out constant variables, using the standard deviation from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Fit Summary------\n",
      "---------------------\n",
      "A total of 266 features are not constant\n",
      "A total of 34 features are constant\n",
      "The train shape before fit is (35000, 300)\n",
      "The train shape after fit is (15000, 266)\n",
      "The test shape before fit is (15000, 300)\n",
      "The test shape after fit is (35000, 266)\n"
     ]
    }
   ],
   "source": [
    "obj1.pandas_std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how by removing constant features, we managed to reduced the feature space quite a bit.\n",
    "\n",
    "Both the VarianceThreshold and the snippet of code I provided work with numerical variables. What can we do to find constant categorical variables?\n",
    "\n",
    "One alternative is to encode the categories as numbers and then use the code above. But then you will put effort in pre-processing variables that are not informative.\n",
    "\n",
    "The code below offers a better solution:\n",
    "\n",
    "### Manual Code 2 - works also with categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Fit Summary------\n",
      "---------------------\n",
      "A total of 266 features are not constant\n",
      "A total of 34 features are constant\n",
      "The train shape before fit is (35000, 300)\n",
      "The train shape after fit is (15000, 266)\n",
      "The test shape before fit is (15000, 300)\n",
      "The test shape after fit is (35000, 266)\n"
     ]
    }
   ],
   "source": [
    "obj1.pandas_nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
